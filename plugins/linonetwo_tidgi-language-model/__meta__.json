{"title":"$:/plugins/linonetwo/tidgi-language-model","author":"LinOnetwo","name":"Tidgi Language Model","description":"Chat with TidGi's build-in language model service (LLama/Rwkv) in Tiddlywiki. A private, local and rooted ChatGPT AI.","readme":"<$button class=\"tc-btn-invisible\" style=\"overflow: hidden;white-space: pre;width: 100%;\" message=\"tm-open-external-window\" param=\"https://github.com/tiddly-gittly/tidgi-language-model/\">{{$:/core/images/github}} <$text text=\"https://github.com/tiddly-gittly/tidgi-language-model/\"/></$button>\n\n<$button class=\"tc-btn-invisible\" style=\"overflow: hidden;white-space: pre;width: 100%;\" message=\"tm-open-external-window\" param=\"https://tiddly-gittly.github.io/tidgi-language-model/\">{{$:/core/images/home-button}} <$text text=\"https://tiddly-gittly.github.io/tidgi-language-model/\"/></$button>\n\nUsing LLaMa in TiddlyWiki.\n\nYou will have an additional \"TG AI\" page in your sidebar, where you can have a conversation directly, and the history of the conversation will be saved. To clear the history, simply delete the entry pointed to by the `history` parameter.\n\nLLaMa is actually a widget that allows you to customize the chatbot according to your needs:\n\n```html\n<$tidgi-chat />\n```\n\nVarious optional parameters can also be added to customize the behavior.\n\n|!Attributes |!Explanation |\n|history |Fill in an tiddler title for persistent storage of chat logs |\n|scroll |If yes, the conversation record can be scrolled up and down, but the height must be specified in the outer layer of the widget, refer to the [[sidebar|$:/plugins/linonetwo/tidgi-language-model/side-bar]] writing |\n|component |DOM tag type for microware, default is div |\n|className |Class name of the widget for custom styles |\n|readonly |If it is readonly, no dialog input box will appear, and it will be used for display only with the history parameter. |\n|system_message |System messages to customize the AI's behavior, such as \"You are an experienced lawyer\" |\n\nIn addition, the following LLaMa parameters are supported:\n\n* nThreads\n* nTokPredict\n* repeatPenalty\n* temp\n* topK\n* topP\n\nIts specific usage can check the [[official documentation|https://llama-node.vercel.app/docs/backends/llama.cpp/inference]].\n\nNow there is no multi-round dialogue, even in a micro-piece chat, but also a single round of dialogue, multi-round dialogue and so on the next version to engage.\n\n!! Advance\n\nIf you nest your own action in the widget, you can get the result of the answer when the conversation is completed, which requires that you know how to write a widget that supports actions. The output is stored in the `output-text` variable.\n\nAt the same time, you can also catch bubbling events of the widget when the conversation completes, as well as global events, both using `addEventListener` and `$tw.hooks.addHook` (the event name is `tidgi-chat`) respectively. The following is the type definition of the event load.\n","version":"0.3.1","plugin-type":"plugin","requires-reload":false,"category":"Unknown","type":"application/json","latest":"0.3.1","versions":["0.2.0","0.3.0","0.3.1"],"versions-size":{"0.2.0":29319,"0.3.0":35589,"0.3.1":44702}}